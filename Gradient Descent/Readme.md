<h1><b>The Vanilla Gradient Descent / Batch Gradient Descent </b> </h1>
<h2> Optimization Goals </h2>
<p>Optimization refers maily to two basic concepts :-</p>
<p>1.Minimization</p>
<p>2.Maximization</p>
<p>of the objective function <b>f(x)</b> parameterised by our independent variable x. In the Machine/Deep Learning terminology we are endowed with task of minimizing the loss function <b>L(w)</b> parameterised by the model parametres .</p>
<p align="center">
  <img src="https://github.com/Nilotpal1998/Optimization-Algorithms/blob/main/Gradient%20Descent/gradient_images/chart1.jpg" width="550" title="Goal of Optimization">
</p>
<p>Mainly Optimization Algorithms have been endowed with the following Goals:-</p>
<p>1.Reaching the <b>Global Optima</b> </p>
<p>2. Reaching a <b>Local Optima </b></p>
<p>Reaching the global optima is possible generally if the function is convex i.e any local minima is global minima . Whereas we are satisfied to be at the local minima of a neighbourhood if we are having the non convex scenario .</p>
